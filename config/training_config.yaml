# YOLOv8 Training Configuration
# Hyperparameters for finetuning the Albion Online resource detection model

# Model configuration
model: model.pt  # Path to pretrained model (or yolov8n.pt for scratch)
data: config/dataset.yaml  # Dataset configuration

# Training hyperparameters
epochs: 100
batch: 16  # Reduce to 8 if OOM errors occur
imgsz: 640  # Input image size

# Device
device: 0  # GPU device (0, 1, 2, etc.) or 'cpu'
workers: 8  # Number of dataloader workers

# Optimizer
optimizer: SGD  # SGD, Adam, AdamW, NAdam, RAdam, RMSProp
lr0: 0.01  # Initial learning rate
lrf: 0.01  # Final learning rate (lr0 * lrf)
momentum: 0.937  # SGD momentum/Adam beta1
weight_decay: 0.0005  # Optimizer weight decay
warmup_epochs: 3.0  # Warmup epochs
warmup_momentum: 0.8  # Warmup initial momentum
warmup_bias_lr: 0.1  # Warmup initial bias lr

# Data augmentation
augment: true
hsv_h: 0.015  # Image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # Image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # Image HSV-Value augmentation (fraction)
degrees: 0.0  # Image rotation (+/- deg)
translate: 0.1  # Image translation (+/- fraction)
scale: 0.5  # Image scale (+/- gain)
shear: 0.0  # Image shear (+/- deg)
perspective: 0.0  # Image perspective (+/- fraction)
flipud: 0.0  # Image vertical flip probability
fliplr: 0.5  # Image horizontal flip probability
mosaic: 1.0  # Mosaic augmentation probability
mixup: 0.0  # Mixup augmentation probability
copy_paste: 0.0  # Copy-paste augmentation probability

# Loss weights
box: 7.5  # Box loss gain
cls: 0.5  # Class loss gain
dfl: 1.5  # DFL loss gain

# Validation
val: true  # Validate during training
val_period: 1  # Validation every N epochs
save_period: 10  # Save checkpoint every N epochs
patience: 50  # Early stopping patience (epochs)

# Output
project: runs/train
name: exp
exist_ok: true
pretrained: true
verbose: true
plots: true

# Advanced
close_mosaic: 10  # Disable mosaic augmentation for final N epochs
amp: true  # Automatic Mixed Precision training
fraction: 1.0  # Fraction of dataset to train on
profile: false  # Profile ONNX and TensorRT speeds
freeze: null  # Freeze layers (e.g., [0, 1, 2])
multi_scale: false  # Multi-scale training
overlap_mask: true  # Masks should overlap during training
mask_ratio: 4  # Mask downsample ratio
dropout: 0.0  # Use dropout regularization
seed: 0  # Random seed for reproducibility

# Notes for finetuning:
# - Start with learning rate 0.001-0.01 for finetuning
# - Use larger learning rate (0.01) for training from scratch
# - Increase batch size if you have more GPU memory
# - Enable augmentation for better generalization
# - Adjust patience for early stopping based on dataset size
# - Use warmup for stable training start
